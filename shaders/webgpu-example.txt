/**
 * At time of writing, the web-gpu spec is really *just* coming around to finalization. While I'd
 * love to upgrade the clunky CPU-driven calcs for item effects, I don't want to commit to webgl2,
 * which is on the way out. At the same time, PixiJS announced one week before this writing that
 * they are releasing a beta version of their WebGPU-implmenting v8.
 */

const runWebGPUShaders = async () => {
  const adapter = await navigator.gpu.requestAdapter();
  if (!adapter) {
    console.debug('couldn\'t get the adapter, even though we support GPU!');
    return;
  }

  const device = await adapter.requestDevice();
  const context = canvasRef.current!.getContext('webgpu');
  if (!context) {
    console.debug('couldn\'t get the webGPU context for the canvas');
    return;
  }

  const format = navigator.gpu.getPreferredCanvasFormat();
  context.configure({
    device,
    format,
    // Only saw this in MDN tutorial, not in the Chrome one.
    alphaMode: 'premultiplied',
  });

  const code = `
  @vertex fn vertexMain(
    @builtin(vertex_index) i: u32
  ) ->
    @builtin(position) vec4f {
    const pos = array(vec2f(0, 1), vec2f(-1, -1), vec2f(1, -1));
    return vec4f(pos[i], 0, 1);
  }
  
  @fragment fn fragmentMain() ->
    @location(0) vec4f {
    return vec4f(1, 0, 0, 1);
  }
  `;

  const code2 = `
  struct VertexOut {
    @builtin(position) position: vec4f,
    @location(0) color: vec4f
  }

  @vertex
  fn vertexMain(
    @location(0) position: vec4f,
    @location(1) color: vec4f
  ) -> VertexOut
  {
    var output: VertexOut;
    output.position = position;
    output.color = color;
    return output;
  }

  @fragment
  fn fragmentMain(fragData: VertexOut) ->
  @location(0) vec4f
  {
    return fragData.color;
  }
  `;

  // ---- Start of input param actions ----
  const code2Vertices = new Float32Array([
    -0.5, -0.5, +0.0, 1, 1, 0, 0, 1,
    +0.0, +0.4, +0.0, 1, 0, 1, 0, 1,
    +0.5, -0.5, +0.0, 1, 0, 0, 1, 1,
  ]);

  const code2VertexBuffer = device.createBuffer({
    size: code2Vertices.byteLength,
    usage: GPUBufferUsage.VERTEX | GPUBufferUsage.COPY_DST,
  });

  device.queue.writeBuffer(
    code2VertexBuffer,
    0,
    code2Vertices,
    0,
    code2Vertices.length,
  );

  const code2VertexBuffers: GPURenderPipelineDescriptor['vertex']['buffers'] = [
    {
      attributes: [
        {
          shaderLocation: 0, // position
          offset: 0,
          format: 'float32x4',
        },
        {
          shaderLocation: 1, // color
          offset: 16,
          format: 'float32x4',
        },
      ],
      arrayStride: 32,
      stepMode: 'vertex',
    }
  ];

  const colorAttachments2: GPURenderPassDescriptor['colorAttachments'] = [
    {
      clearValue: { r: 0.0, g: 0.0, b: 0.0, a: 0.0 },
      loadOp: 'clear',
      storeOp: 'store',
      view: context.getCurrentTexture().createView(),
    },
  ];
  // ---- End of input param actions ----

  const shaderModule = device.createShaderModule({
    code: code2,
  });

  const pipeline = device.createRenderPipeline({
    layout: 'auto',
    vertex: {
      module: shaderModule,
      entryPoint: 'vertexMain',
      buffers: code2VertexBuffers,
    },
    fragment: {
      module: shaderModule,
      entryPoint: 'fragmentMain',
      targets: [{ format }]
    },
  });

  const commandEncoder = device.createCommandEncoder();
  const colorAttachments: GPURenderPassDescriptor['colorAttachments'] = [
    {
      view: context.getCurrentTexture().createView(),
      loadOp: 'clear',
      storeOp: 'store',
    },
  ];

  const passEncoder = commandEncoder.beginRenderPass({
    colorAttachments: colorAttachments2,
  });
  passEncoder.setPipeline(pipeline);
  // --- more stuff for vars
  passEncoder.setVertexBuffer(0, code2VertexBuffer);
  // --- end more stuff for vars
  passEncoder.draw(3);
  passEncoder.end();
  device.queue.submit([commandEncoder.finish()]);
};